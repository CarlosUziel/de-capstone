{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import sys, os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from IPython import display as ICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import process_config\n",
    "from utils.aws import create_s3_bucket\n",
    "from utils.spark import create_spark_session\n",
    "from data.tables import (\n",
    "    ON_LOAD_TABLES_SCHEMA,\n",
    "    ON_LOAD_TABLES_FILES,\n",
    "    STAR_EXTRACT_TABLES_ARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config, dl_config = (\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"_user.cfg\")),\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"dl.cfg\")),\n",
    ")\n",
    "spark = create_spark_session(user_config, dl_config)\n",
    "s3_bucket_prefix = dl_config.get(\"S3\", \"BUCKET_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Preview raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table_schema in ON_LOAD_TABLES_SCHEMA.items():\n",
    "    table_paths = ON_LOAD_TABLES_FILES[table_name]\n",
    "    table_df = spark.read.csv(\n",
    "        (\n",
    "            str(table_paths)\n",
    "            if not isinstance(table_paths, Iterable)\n",
    "            else [str(p) for p in table_paths]\n",
    "        ),\n",
    "        schema=ON_LOAD_TABLES_SCHEMA[table_name],\n",
    "        header=True,\n",
    "    )\n",
    "\n",
    "    n_elem = table_df.count()\n",
    "    table_df_preview = spark.createDataFrame(\n",
    "        table_df.take(5),\n",
    "        schema=ON_LOAD_TABLES_SCHEMA[table_name],\n",
    "    ).toPandas()\n",
    "\n",
    "    print(f\"First 5 rows of {table_name}:\")\n",
    "    print(f\"Columns: {table_df.columns}.\")\n",
    "    ICD.display(table_df_preview)\n",
    "    print(f\"The full table contains a total of {n_elem} records\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Run ETL pipeline to extract STAR dimensional tables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create S3 bucket to store all results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert create_s3_bucket(user_config, dl_config), \"Error creating S3 bucket.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Airflow DAG (`capstone_etl`) now.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run analytics queries on dimensional tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_path = Path(\"../data\").joinpath(\"profiling_reports\")\n",
    "profiling_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data profiling of dimensional tables\n",
    "\n",
    "WARNING: Avoid for tables with numbers of rows in the order of dozens of millions, according to memory availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table_args in STAR_EXTRACT_TABLES_ARGS.items():\n",
    "    star_table = spark.read.parquet(table_args[\"op_kwargs\"][\"s3_save_path\"]).toPandas()\n",
    "    ProfileReport(star_table).to_file(profiling_path.joinpath(f\"{table_name}.html\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Example queries using combinations of dimensional tables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do immigrants prefer destinations with higher or lower population?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do immigrants prefer destinations with higher or lower temperature?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do immigrants prefer destinations with higher or less airports?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "- Propose how often the data should be updated and why.\n",
    "- Write a description of how you would approach the problem differently under the following scenarios:\n",
    "  - The data was increased by 100x.\n",
    "  - The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "  - The database needed to be accessed by 100+ people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('de_capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c6ab47c24cf80276cd6370728e56e74687e2bfe799d1173d929047132a801f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

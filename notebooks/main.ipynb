{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import sys, os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from IPython import display as ICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import process_config\n",
    "from utils.aws import create_s3_bucket\n",
    "from utils.spark import create_spark_session\n",
    "from data.tables import ON_LOAD_TABLES_SCHEMA, ON_LOAD_TABLES_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config, dl_config = (\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"_user.cfg\")),\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"dl.cfg\")),\n",
    ")\n",
    "spark = create_spark_session(user_config, dl_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Preview raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table_schema in ON_LOAD_TABLES_SCHEMA.items():\n",
    "    table_paths = ON_LOAD_TABLES_FILES[table_name]\n",
    "    table_df = spark.read.csv(\n",
    "        (\n",
    "            str(table_paths)\n",
    "            if not isinstance(table_paths, Iterable)\n",
    "            else [str(p) for p in table_paths]\n",
    "        ),\n",
    "        schema=ON_LOAD_TABLES_SCHEMA[table_name],\n",
    "        header=True,\n",
    "    )\n",
    "\n",
    "    n_elem = table_df.count()\n",
    "    table_df_preview = spark.createDataFrame(\n",
    "        table_df.take(5),\n",
    "        schema=ON_LOAD_TABLES_SCHEMA[table_name],\n",
    "    ).toPandas()\n",
    "\n",
    "    print(f\"First 5 rows of {table_name}:\")\n",
    "    ICD.display(table_df_preview)\n",
    "    print(f\"The full table contains a total of {n_elem} records\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Run ETL pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create S3 bucket for clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert create_s3_bucket(user_config, dl_config), \"Error creating S3 bucket.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigger Airflow DAG here ([reference](https://stackoverflow.com/questions/60055151/how-to-trigger-an-airflow-dag-run-from-within-a-python-script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run analytics queries on dimensional tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "#### Explore the Data\n",
    "\n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "Document steps necessary to clean the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### Step 3: Define the Data Model\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "List the steps necessary to pipeline the data into the chosen data model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### Step 4: Run Pipelines to Model the Data\n",
    "\n",
    "#### 4.1 Create the data model\n",
    "\n",
    "Build the data pipelines to create the data model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    "\n",
    "- Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "- Unit tests for the scripts to ensure they are doing the right thing\n",
    "- Source/Count checks to ensure completeness\n",
    "\n",
    "Run Quality Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary\n",
    "\n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "- Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "- Propose how often the data should be updated and why.\n",
    "- Write a description of how you would approach the problem differently under the following scenarios:\n",
    "- The data was increased by 100x.\n",
    "- The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "- The database needed to be accessed by 100+ people.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('de_capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c6ab47c24cf80276cd6370728e56e74687e2bfe799d1173d929047132a801f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering Capstone Project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import sys, os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "from IPython import display as ICD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path: str = \"../src\"\n",
    "sys.path.append(src_path)\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.io import process_config\n",
    "from utils.aws import create_s3_bucket\n",
    "from utils.spark import create_spark_session\n",
    "from data.tables import (\n",
    "    ON_LOAD_TABLES_SCHEMA,\n",
    "    ON_LOAD_TABLES_FILES,\n",
    "    STAR_EXTRACT_TABLES_ARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_config, dl_config = (\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"_user.cfg\")),\n",
    "    process_config(Path(os.getcwd()).parent.joinpath(\"dl.cfg\")),\n",
    ")\n",
    "spark = create_spark_session(user_config, dl_config)\n",
    "s3_bucket_prefix = dl_config.get(\"S3\", \"BUCKET_NAME\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Preview raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table_schema in ON_LOAD_TABLES_SCHEMA.items():\n",
    "    table_paths = ON_LOAD_TABLES_FILES[table_name]\n",
    "    table_df = spark.read.csv(\n",
    "        (\n",
    "            str(table_paths)\n",
    "            if not isinstance(table_paths, Iterable)\n",
    "            else [str(p) for p in table_paths]\n",
    "        ),\n",
    "        schema=ON_LOAD_TABLES_SCHEMA[table_name],\n",
    "        header=True,\n",
    "    )\n",
    "\n",
    "    n_elem = table_df.count()\n",
    "    table_df_preview = spark.createDataFrame(\n",
    "        table_df.take(5),\n",
    "        schema=ON_LOAD_TABLES_SCHEMA[table_name],\n",
    "    ).toPandas()\n",
    "\n",
    "    print(f\"First 5 rows of {table_name}:\")\n",
    "    print(f\"Columns: {table_df.columns}.\")\n",
    "    ICD.display(table_df_preview)\n",
    "    print(f\"The full table contains a total of {n_elem} records\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Run ETL pipeline to extract STAR dimensional tables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create S3 bucket to store all results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert create_s3_bucket(user_config, dl_config), \"Error creating S3 bucket.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Airflow DAG (`capstone_etl`) now.\n",
    "![Capstone DAG](../images/capstone_dag.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Run analytics queries on dimensional tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_path = Path(\"../data\").joinpath(\"profiling_reports\")\n",
    "profiling_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_tables = {\n",
    "    table_name: spark.read.parquet(table_args[\"op_kwargs\"][\"s3_save_path\"])\n",
    "    for table_name, table_args in STAR_EXTRACT_TABLES_ARGS.items()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data profiling of dimensional tables\n",
    "\n",
    "WARNING: Avoid for tables with numbers of rows in the order of dozens of millions, according to memory availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, table_df in star_tables.items():\n",
    "    if table_name == \"fact_immigration\":\n",
    "        continue\n",
    "    star_table = table_df.toPandas()\n",
    "    ProfileReport(star_table).to_file(profiling_path.joinpath(f\"{table_name}.html\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Example queries using combinations of dimensional tables\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do immigrants prefer destinations with higher or lower population?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    star_tables[\"fact_immigration\"]\n",
    "    .groupBy(\"city_id\")\n",
    "    .count()\n",
    "    .join(\n",
    "        star_tables[\"fact_us_demogr\"],\n",
    "        (\n",
    "            star_tables[\"fact_immigration\"][\"city_id\"]\n",
    "            == star_tables[\"fact_us_demogr\"][\"city_id\"]\n",
    "        ),\n",
    "    )\n",
    "    .dropna(subset=[\"total_population\"])\n",
    "    .select([\"count\", \"total_population\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"count\", \"total_population\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high positive correlation between the population size of a city and the number of immigrants it attracts.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do immigrants prefer destinations with higher or lower temperature?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    star_tables[\"fact_immigration\"]\n",
    "    .groupBy(\"city_id\")\n",
    "    .count()\n",
    "    .join(\n",
    "        star_tables[\"fact_temps\"],\n",
    "        (\n",
    "            star_tables[\"fact_immigration\"][\"city_id\"]\n",
    "            == star_tables[\"fact_temps\"][\"city_id\"]\n",
    "        ),\n",
    "    )\n",
    "    .dropna(subset=[\"avg_temperature\"])\n",
    "    .select([\"count\", \"avg_temperature\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"count\", \"avg_temperature\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no correlation between number of immigrants and average temperature of a city.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do immigrants prefer destinations with more or less airports?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    star_tables[\"fact_immigration\"]\n",
    "    .groupBy(\"city_id\")\n",
    "    .count()\n",
    "    .join(\n",
    "        (\n",
    "            star_tables[\"dim_airports\"]\n",
    "            .groupBy(\"city_id\")\n",
    "            .count()\n",
    "            .withColumnRenamed(\"count\", \"airports_count\")\n",
    "        ),\n",
    "        (\n",
    "            star_tables[\"fact_immigration\"][\"city_id\"]\n",
    "            == star_tables[\"dim_airports\"][\"city_id\"]\n",
    "        ),\n",
    "    )\n",
    "    .select([\"count\", \"airports_count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.corr(\"count\", \"airports_count\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a low, positive correlation between number of immigrants and number of airports in the receiving city."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('de_capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c6ab47c24cf80276cd6370728e56e74687e2bfe799d1173d929047132a801f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
